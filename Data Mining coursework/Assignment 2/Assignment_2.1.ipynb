{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Assignment_2.1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QFTN7dqWRf4P"},"source":["# Data Mining Lab Week 5: Data Warehousing and On-Line Analytical Processing (OLAP)"]},{"cell_type":"markdown","metadata":{"id":"c8nEIoi9Rf4Q"},"source":["## Introduction \n","\n","The aim of this lab is for students to get experience with **Data Warehousing** and **On-line Analytical Processing (OLAP)** covered in week 5, and more specifically with the concepts of **data cubes**, **data cube measures**, **typical OLAP operations**, and **data cube computation**.\n","\n","- This lab is the first part of a **two-week assignment** that covers weeks 5 and 6.\n","- This lab corresponds to the **second assignment**, accounting for 10% of your overall grade. Questions in this lab sheet will contribute to 5% of your overall grade; questions in the lab sheet for week 6 will cover for another 5% of your overall grade.\n","- <font color = 'maroon'>The last section of this notebook includes the questions that are assessed towards your final grade.</font> "]},{"cell_type":"markdown","metadata":{"id":"0e-lRsp_Rf4R"},"source":["## Important notes about the assignment: \n","\n","- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n","- The total assessed coursework is worth 40% of your final grade.\n","- There will be 9 lab sessions and 4 assignments.\n","- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n","- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n","- You are asked to submit a **report** that should answer the questions specified in the last section of this notebook. The report should be in **PDF format** (so **NOT** *doc, docx, notebook* etc). It should be well identified with your name, student number, assignment number (for instance, Assignment 1), module, and marked with question numbers. \n","- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n","- Please name your report as follows: Assignment1-StudentName-StudentNumber.pdf\n","- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered."]},{"cell_type":"markdown","metadata":{"id":"JsnNFY7KRf4S"},"source":["## 1. Introduction to Cubes\n","\n","This chapter describes step-by-step how to use Cubes (http://cubes.databrewery.org/), a lightweight Python framework and set of tools for development of reporting and analytical applications, Online Analytical Processing (OLAP), multidimensional analysis and browsing of aggregated data. We will be working with v1.1 of Cubes. Cubes features:\n","- a logical view of analysed data - how analysts look at data, how they think of data, not not how the data are physically implemented in the data stores\n","- OLAP and aggregated browsing (default backend is for relational databse - ROLAP)\n","- hierarchical dimensions (attributes that have hierarchical dependencies, such as category-subcategory or country-region)\n","- multiple hierarchies in a dimension\n","- localizable metadata and data (see Localization)\n","- authentication and authorization of cubes and their data\n","- pluggable data warehouse – plug-in other cube-like (multidimensional) data sources\n","\n","Cubes is meant to be used by application builders that want to provide analytical functionality. Cubes also relies on methods from SQLAlchemy (https://www.sqlalchemy.org/), an open-source SQL toolkit and object-relational mapper for Python."]},{"cell_type":"markdown","metadata":{"id":"uVIn0LJIRf4S"},"source":["## 2. Data Preparation\n","\n","The example data used here are International Bank for Reconstruction and Development (IBRD) Balance Sheet. For this example we will be using the CSV file \"IBRD_Balance_Sheet__FY2010.csv\" which is provided in the supplementary material for the lab. The CSV file includes records which are characterised by a Category (and subcategories), Line Item, Fiscal Year, and Amount (in US$ millions). We first start with imports:"]},{"cell_type":"code","metadata":{"id":"VL6oIW7eRf4T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604405538085,"user_tz":0,"elapsed":6803,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"1016cb6c-5c8a-40fa-d63a-f746822b984e"},"source":["# If you are using Google Colab, you would need to run the below line to install Cubes. \n","# The below line might not be needed if you are running a local python installation with Cubes installed.\n","!pip install cubes\n","\n","from sqlalchemy import create_engine\n","from cubes.tutorial.sql import create_table_from_csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting cubes\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/78/cefca0763d3042ddfa0cd463cb7464dad11b75aefdf9fca6c6bb0dce9416/cubes-1.1.tar.gz (128kB)\n","\r\u001b[K     |██▌                             | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from cubes) (2.8.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from cubes) (2.6.0)\n","Collecting expressions>=0.2.3\n","  Downloading https://files.pythonhosted.org/packages/88/92/0c83b5d642a387ffef2696c7a4eeb54e5f8e6eea1d25a07c0309579a308c/expressions-0.2.3.tar.gz\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->cubes) (1.15.0)\n","Collecting grako>=3.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/1d/c10ddc7ccf060695d1cefda9726a2923d45e7a4d39ac08ede3b014e9b33f/grako-3.99.9-py2.py3-none-any.whl (82kB)\n","\u001b[K     |████████████████████████████████| 92kB 6.5MB/s \n","\u001b[?25hBuilding wheels for collected packages: cubes, expressions\n","  Building wheel for cubes (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cubes: filename=cubes-1.1-cp36-none-any.whl size=151591 sha256=2ca9b733147c1d7817ca27e8a2f7fc37ab0e083af9da52d736f40f561a3844a0\n","  Stored in directory: /root/.cache/pip/wheels/1f/89/09/c361b647a6cc65335af4bc1f27b07abd123727f429819d2590\n","  Building wheel for expressions (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for expressions: filename=expressions-0.2.3-cp36-none-any.whl size=6445 sha256=f79270aadd799b965199d3c178be99a20897ccd8e81addba3733b8b57b03d460\n","  Stored in directory: /root/.cache/pip/wheels/56/67/97/1a31fbf02237daec61a3d75439d782cf13dfc4af31e8b4e80e\n","Successfully built cubes expressions\n","Installing collected packages: grako, expressions, cubes\n","Successfully installed cubes-1.1 expressions-0.2.3 grako-3.99.9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-nQJKMgnRf4X"},"source":["We can now load the data, create a table and populate it with contents of the CSV file:"]},{"cell_type":"code","metadata":{"id":"AkUt3RD0Rf4X"},"source":["engine = create_engine('sqlite:///data.sqlite')\n","create_table_from_csv(engine,\n","                      \"IBRD_Balance_Sheet__FY2010.csv\",\n","                      table_name=\"ibrd_balance\",\n","                      fields=[\n","                          (\"category\", \"string\"),\n","                          (\"category_label\", \"string\"),\n","                          (\"subcategory\", \"string\"),\n","                          (\"subcategory_label\", \"string\"),\n","                          (\"line_item\", \"string\"),\n","                          (\"year\", \"integer\"),\n","                          (\"amount\", \"integer\")],\n","                      create_id=True\n","                     )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jy_FKwmKRf4a"},"source":["## 3. Creating a data cube\n","\n","Everything in Cubes happens in an *analytical workspace*. It contains cubes, maintains connections to the data stores (with cube data), provides connection to external cubes and more.\n","\n","The workspace properties are specified in a configuration file slicer.ini (default name). The first thing we have to do is to specify a data store – a database which will host the cube’s data:"]},{"cell_type":"code","metadata":{"id":"vNUECSZqRf4a"},"source":["from cubes import Workspace\n","\n","workspace = Workspace()\n","workspace.register_default_store(\"sql\", url=\"sqlite:///data.sqlite\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEzCwcdqRf4d"},"source":["The structure of data cubes (in terms of dimensions, measures, and aggregates) is specified in JSON files. We now import file 'tutorial_model.json' (found in the lab supplementary material) which includes an example model of the data cube, dimension tables, and aggregate functions for the CSV file we loaded previously."]},{"cell_type":"code","metadata":{"id":"5L1zCE38Rf4d"},"source":["workspace.import_model(\"tutorial_model.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5mvdW24Rf4g"},"source":["**Please make sure to inspect the structure of the above JSON file - this will be relevant for one of the assignment questions.**\n","\n","We can now create a data cube based on the above data cube model and data table:"]},{"cell_type":"code","metadata":{"id":"GiWbuaEZRf4g"},"source":["cube = workspace.cube(\"ibrd_balance\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rn25hJmGRf4i"},"source":["## 4. Aggregations and OLAP operations\n","\n","*Browser* is an object that does the actual aggregations and other data queries for a cube. To obtain one:"]},{"cell_type":"code","metadata":{"id":"ygBQkH62Rf4j"},"source":["browser = workspace.browser(cube)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-TWgJrORf4m"},"source":["We can now compute aggregates of the data cube as specified by the data cube model. For computing the total count of records:"]},{"cell_type":"code","metadata":{"id":"KtD8x74PRf4m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604399933435,"user_tz":0,"elapsed":627,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"15ea1d65-f906-4238-c729-5bfdb66fecc0"},"source":["result = browser.aggregate()\n","result.summary[\"record_count\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"MLWt_tmdRf4o"},"source":["For computing a sum of the amount:"]},{"cell_type":"code","metadata":{"id":"vcfVXMclRf4p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604398845791,"user_tz":0,"elapsed":2966,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"6be41a53-1d92-45ea-b131-bee9a021196e"},"source":["result.summary[\"amount_sum\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1116860"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"u73x-LvWRf4r"},"source":["Now we can try to compute aggregates by year:"]},{"cell_type":"code","metadata":{"id":"zi-gKfTRRf4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604400552578,"user_tz":0,"elapsed":638,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"cbcd8e07-929a-4c02-83bc-fc45e38fc0db"},"source":["result = browser.aggregate(drilldown=[\"year\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'year': 2009, 'amount_sum': 550840, 'record_count': 31, 'amount_min': -1683, 'amount_max': 110040}\n","{'year': 2010, 'amount_sum': 566020, 'record_count': 31, 'amount_min': -3043, 'amount_max': 128577}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4xkpjJiVRf4t"},"source":["Or compute aggregates by item category:"]},{"cell_type":"code","metadata":{"id":"yJJSHD3KRf4u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604398845792,"user_tz":0,"elapsed":2335,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"2948f765-12f8-4a99-9cd6-52e89d1ad895"},"source":["result = browser.aggregate(drilldown=[\"item\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 558430, 'record_count': 32}\n","{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 77592, 'record_count': 8}\n","{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 480838, 'record_count': 22}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jS00Ack2Rf4x"},"source":["We can also perform *slicing* and *dicing* operations on the data cube. The below example performs a slicing operation on the data cube by selecting only entries with the year being 2009, and displays aggregates according to the item category. Here, a *cell* defines a point of interest – portion of the cube to be aggergated or browsed."]},{"cell_type":"code","metadata":{"id":"XTTeoPIYRf4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604398845793,"user_tz":0,"elapsed":2008,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"57843796-734a-4155-c0d3-d6e3f7040588"},"source":["import cubes as cubes\n","cuts = [cubes.PointCut(\"year\", [\"2009\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell, drilldown=[\"item\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 275420, 'record_count': 16}\n","{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 40037, 'record_count': 4}\n","{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 235383, 'record_count': 11}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"533Hl-LkRf40"},"source":["It's worth noting that in Cubes, slicing operations can be created by either specifying a \"point cut\" which selects a single value of an attribute in a given dimension (called using the cubes.PointCut() function as above), or by specifying a \"range cut\", which selects a range of values for a given dimension. The range cut can be called using the cubes.RangeCut() function, which takes as input the attribute name, the minimum value of the specified range, and the maximum value of the range.\n","\n","Similarly, we can perform a *dicing* operation on the data cube by performing a selection on two or more dimensions. The below example performs a dicing operation on the data cube, selecting entries with the year being 2009 and the item category being \"a\", and displays the aggregate results:"]},{"cell_type":"code","metadata":{"id":"V2wGgjO_Rf40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604398846060,"user_tz":0,"elapsed":2001,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"77310032-f7d2-4be0-cb6f-c8697bd4f089"},"source":["cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell,drilldown=[\"item\"])\n","result.summary"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'amount_sum': 275420, 'record_count': 16}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"m4QhV35ORf42"},"source":["We can also *drill down* lower in the Category hierarchy. Here, we perform a dicing operation to select records with year being 2009 and item category being \"a\" (corresponding to assets), and show aggregates for each subcategory level."]},{"cell_type":"code","metadata":{"id":"QMkfXBH6Rf43","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604398846062,"user_tz":0,"elapsed":1683,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"184a2bf0-6091-49be-d8d9-7049e82c6f0a"},"source":["cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell,drilldown=[\"item:subcategory\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'da', 'item.subcategory_label': 'Derivative Assets', 'amount_sum': 123065, 'record_count': 4}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'dfb', 'item.subcategory_label': 'Due from Banks', 'amount_sum': 3044, 'record_count': 2}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'i', 'item.subcategory_label': 'Investments', 'amount_sum': 41012, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'lo', 'item.subcategory_label': 'Loans Outstanding', 'amount_sum': 103657, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'nn', 'item.subcategory_label': 'Nonnegotiable', 'amount_sum': 1202, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'oa', 'item.subcategory_label': 'Other Assets', 'amount_sum': 2247, 'record_count': 3}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'orcv', 'item.subcategory_label': 'Other Receivables', 'amount_sum': 984, 'record_count': 2}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'rcv', 'item.subcategory_label': 'Receivables', 'amount_sum': 176, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 's', 'item.subcategory_label': 'Securities', 'amount_sum': 33, 'record_count': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T58EDi6eRf45"},"source":["## <font color = 'maroon'>Assignment 2 (part 1/2)</font>\n","\n","1. A data warehouse for a music streaming company consists of the dimensions song, user, time (time and date of when the user listened to a song), and the two measures count (how many times a user listened to the song) and fee (fee paid by the streaming company to the artist every time a user listens to that song). \n","  1. Draw a schema diagram for the above data warehouse using either a star, snowflake, or fact constellation schema. [0.5 marks out of 5]\n","  2. Starting with the base cuboid [time, user, song], what specific OLAP operations should be performed in order to list the total fee collected for a given song for a given month of a given year (e.g. October 2020)? [0.5 marks out of 5]\n","  3. Assume that the time dimension has 5 levels (e.g. day, day of week, month, quarter, year); and that the song and user dimensions both have 1 level (not including the virtual level 'all'). How many cuboids will this cube contain (including the base and apex cuboids)? [0.5 marks out of 5]\n","  \n","\n","2. Suppose we have access to a data cube that contains information on rainfall for specific regions; the data cube has dimensions region, precipitation, and time.\n","  1. Assuming that we would like to compute the total amount of rainfall for a given region and month, which *data cube measure* would we use? To which category of data cube measures does this particular measure fall into? [0.25 marks out of 5]\n","  2. Assuming that we would like to compute the average rainfall for a given region and month, which *data cube measure* would we use? To which category of data cube measures does this particular measure fall into? [0.25 marks out of 5]\n","  \n","\n","3. Suppose that a car rental company has a data warehouse that holds record ID lists of vehicles in terms of brands (Audi, Ford, Mini) and store branches (Tower Hamlets, Newham, Hackney). Each record consists of a combination of vehicle brand and branch. We would like to index the OLAP data using bitmap indices. Produce the *base table* for record IDs, and the corresponding *bitmap index tables* for vehicle brand and store branch. [1 mark out of 5]\n","  \n","\n","4. Using the same CSV file and data cube in the above lab tutorial, modify the \"tutorial_model.json\" file to include aggregate measures for the minimum and maximum amount in the data cube. Using these implemented aggregate measures, produce the values for the minimum and maximum amount in the data per year. Make sure to show your workings in the PDF report. [0.5 marks out of 5]\n","\n","\n","5. Using the CSV file \"country-income.csv\" (found in the supplementary lab documents), perform the following:\n","  1. Load the CSV file using Cubes, create a JSON file for the data cube model, and create a data cube for the data. Use as dimensions the region, age, and online shopper fields. Use as measure the income. Define aggregate functions in the data cube model for the total, average, minimum, and maximum income. In your PDF report, show the relevant scripts and files created. [0.5 marks out of 5]\n","  2. Using the created data cube and data cube model, produce aggregate results for: the whole data cube; results per region; results per online shopping activity; and results for all people aged between 40 and 50. [1 mark out of 5]\n","  "]},{"cell_type":"markdown","metadata":{"id":"UWTcynP1y8E9"},"source":["# Q5"]},{"cell_type":"markdown","metadata":{"id":"HWqTgTnNDUdv"},"source":["#### 5.1"]},{"cell_type":"code","metadata":{"id":"2IQNvr-Hy-DX","executionInfo":{"status":"error","timestamp":1609879083373,"user_tz":0,"elapsed":1514,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"579802ba-9819-4662-82f8-0b49ad5c566f","colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["\n","#Loading the data:\n","engine = create_engine('sqlite:///data.sqlite')\n","create_table_from_csv(engine,\n","                      \"country_income.csv\",\n","                      table_name=\"country_income\",\n","                      fields=[\n","                          (\"region\", \"string\"),\n","                          (\"age\", \"integer\"),\n","                          (\"income\", \"integer\"),\n","                          (\"online_shopper\", \"string\")],\n","                      create_id=True\n","                     )"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-510644f50766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Loading the data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sqlite:///data.sqlite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m create_table_from_csv(engine,\n\u001b[1;32m      5\u001b[0m                       \u001b[0;34m\"country_income.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'create_engine' is not defined"]}]},{"cell_type":"code","metadata":{"id":"phgHSxXM2-ls"},"source":["from cubes import Workspace\n","\n","workspace = Workspace()\n","workspace.register_default_store(\"sql\", url=\"sqlite:///data.sqlite\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-_qtypT-y19"},"source":["workspace.import_model(\"country_income.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9qfi5HH-1mT"},"source":["cube = workspace.cube(\"country_income\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwJpL70G8ieI"},"source":["browser = workspace.browser(cube)\n","result = browser.aggregate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvYRQvxj9wDi"},"source":["#### 5.2"]},{"cell_type":"code","metadata":{"id":"ymRjxMvx9Zas","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604419094211,"user_tz":0,"elapsed":517,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"ab98d472-2501-40d1-e123-8569a0f16f1f"},"source":["#Results from whole data-cube\n","print('Total sum:', result.summary[\"income_sum\"])\n","print('Average income:', result.summary[\"income_avg\"])\n","print('Smallest income record:', result.summary[\"income_min\"])\n","print('Largest income record:', result.summary[\"income_max\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total sum: 768200\n","Average income: 76820.0\n","Smallest income record: 57600\n","Largest income record: 99600\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nu025qRaENeX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604419187268,"user_tz":0,"elapsed":523,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"df41aa4d-1c54-4281-a6ed-2b7d4249546c"},"source":["#Per region\n","result = browser.aggregate(drilldown=[\"region\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'region': 'Brazil', 'income_sum': 193200, 'income_avg': 64400.0, 'income_min': 57600, 'income_max': 73200}\n","{'region': 'India', 'income_sum': 331200, 'income_avg': 82800.0, 'income_min': 69600, 'income_max': 94800}\n","{'region': 'USA', 'income_sum': 243800, 'income_avg': 81266.66666666667, 'income_min': 64800, 'income_max': 99600}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5SfqtXDBETxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604419188972,"user_tz":0,"elapsed":503,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"964fc2fd-f483-41a7-c5c2-81e02c2aef0c"},"source":["#Per online shopping activity\n","result = browser.aggregate(drilldown=[\"online_shopper\"])\n","for record in result:\n","    print(record)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'online_shopper': 'No', 'income_sum': 386400, 'income_avg': 77280.0, 'income_min': 62400, 'income_max': 99600}\n","{'online_shopper': 'Yes', 'income_sum': 381800, 'income_avg': 76360.0, 'income_min': 57600, 'income_max': 94800}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1GmCYM0DEbxw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604419219595,"user_tz":0,"elapsed":445,"user":{"displayName":"David von Huth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBiGRa4IHLUi2OiO8AxAcsqavAjCMVDcHZdQmRg-s=s64","userId":"00238832583886889176"}},"outputId":"25e9fdc3-4e8c-4afa-c55c-ee59b691ab3f"},"source":["#For shoppers between age 40-50\n","import cubes as cubes\n","import numpy as np\n","cuts = [cubes.RangeCut(\"age\", [40], [50])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell)\n","\n","print('Total sum:', result.summary[\"income_sum\"])\n","print('Average income:', np.round(result.summary[\"income_avg\"], 2))\n","print('Lowest income:', result.summary[\"income_min\"])\n","print('Highest income:', result.summary[\"income_max\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total sum: 451400\n","Average income: 75233.33\n","Lowest income: 62400\n","Highest income: 86400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LV4HESO5GNC-"},"source":[""],"execution_count":null,"outputs":[]}]}